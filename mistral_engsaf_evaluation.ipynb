{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation & Inference Notebook for Fine-tuned Mistral-7B\n",
        "\n",
        "This notebook contains steps 5-8 from the main training notebook:\n",
        "- **5. Evaluation Metrics** - Comprehensive evaluation of the fine-tuned model\n",
        "- **6. Inference Pipeline** - Grade new student answers\n",
        "- **7. Visualization & Analysis** - Visualize results and analyze performance\n",
        "- **8. Optional Enhancements** - Future enhancements (RAG, Chain-of-Thought, Ensemble)\n",
        "\n",
        "## Usage:\n",
        "1. Set your checkpoint path in the configuration cell\n",
        "2. Run all cells to evaluate your trained model\n",
        "3. Use the inference functions to grade new answers\n",
        "4. Visualize and analyze the results\n",
        "\n",
        "**Note:** This notebook loads the model on a single device (using `model.to(device)`) - NO `device_map='auto'` to avoid device conflicts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries if needed\n",
        "!pip install -q transformers>=4.35.0 peft>=0.6.0 bitsandbytes>=0.41.0 datasets>=2.14.0 accelerate>=0.24.0 scikit-learn nltk rouge-score bert-score torch matplotlib seaborn --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel\n",
        "from sklearn.metrics import (\n",
        "    cohen_kappa_score,\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    print(\"\\nGPU Details:\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        print(f\"  GPU {i}: {props.name}\")\n",
        "        print(f\"    Total Memory: {props.total_memory / 1e9:.2f} GB\")\n",
        "    device = 'cuda:0'  # Use first GPU\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print(\"Warning: No GPU detected. Evaluation will be slow on CPU.\")\n",
        "\n",
        "print(f\"\\nUsing device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - MODIFY THESE PATHS\n",
        "CONFIG = {\n",
        "    # Model paths - UPDATE THESE!\n",
        "    'checkpoint_path': './output/checkpoints',  # Path to your fine-tuned checkpoint\n",
        "    'base_model_name': 'mistralai/Mistral-7B-v0.1',  # Base model name\n",
        "    \n",
        "    # Quantization (should match training config)\n",
        "    'use_4bit': True,\n",
        "    'bnb_4bit_compute_dtype': 'float16',\n",
        "    'bnb_4bit_quant_type': 'nf4',\n",
        "    'use_nested_quant': False,\n",
        "    \n",
        "    # Dataset path\n",
        "    'dataset_dir': 'EngSAF dataset',  # Path to your dataset\n",
        "    \n",
        "    # Evaluation config\n",
        "    'max_length': 1024,\n",
        "    'max_samples': None,  # Set to None to evaluate on all samples, or a number to limit\n",
        "    'temperature': 0.7,\n",
        "    'top_p': 0.9,\n",
        "    'max_new_tokens': 256,\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Fine-tuned Model\n",
        "\n",
        "**IMPORTANT:** This notebook loads the model WITHOUT `device_map='auto'` and uses `model.to(device)` instead to avoid device conflicts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load fine-tuned model (base + LoRA weights)\n",
        "# NO device_map='auto' - we use model.to(device) instead\n",
        "\n",
        "print(f\"Loading fine-tuned model from: {CONFIG['checkpoint_path']}\")\n",
        "print(f\"Base model: {CONFIG['base_model_name']}\")\n",
        "\n",
        "# Configure quantization\n",
        "if CONFIG['use_4bit']:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=CONFIG['bnb_4bit_quant_type'],\n",
        "        bnb_4bit_compute_dtype=getattr(torch, CONFIG['bnb_4bit_compute_dtype']),\n",
        "        bnb_4bit_use_double_quant=CONFIG['use_nested_quant'],\n",
        "    )\n",
        "    print(\"4-bit quantization configured\")\n",
        "else:\n",
        "    bnb_config = None\n",
        "    print(\"Using full precision\")\n",
        "\n",
        "# Load base model (NO device_map='auto')\n",
        "print(\"\\nLoading base model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG['base_model_name'],\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if CONFIG['use_4bit'] else torch.float32\n",
        ")\n",
        "\n",
        "# Load LoRA adapters\n",
        "print(\"Loading LoRA adapters...\")\n",
        "model = PeftModel.from_pretrained(base_model, CONFIG['checkpoint_path'])\n",
        "\n",
        "# Move model to device (single GPU, no device_map issues)\n",
        "print(f\"Moving model to {device}...\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['checkpoint_path'])\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"\\nâœ… Model loaded successfully!\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load EngSAF dataset\n",
        "def load_engsaf_split(dataset_dir=None, split='train'):\n",
        "    \"\"\"Load a specific split of the EngSAF dataset.\"\"\"\n",
        "    if dataset_dir is None:\n",
        "        dataset_dir = CONFIG['dataset_dir']\n",
        "    \n",
        "    split_files = {\n",
        "        'train': 'train.csv',\n",
        "        'val': 'val.csv',\n",
        "        'validation': 'val.csv',\n",
        "        'unseen_question': 'unseen_question.csv',\n",
        "        'unseen_answers': 'unseen_answers.csv',\n",
        "        'test': 'unseen_question.csv'\n",
        "    }\n",
        "    \n",
        "    if split not in split_files:\n",
        "        raise ValueError(f\"Unknown split: {split}\")\n",
        "    \n",
        "    file_path = os.path.join(dataset_dir, split_files[split])\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Could not find {file_path}\")\n",
        "    \n",
        "    df = pd.read_csv(file_path)\n",
        "    \n",
        "    # Map column names\n",
        "    column_mapping = {\n",
        "        'Question': 'question',\n",
        "        'Student Answer': 'student_answer',\n",
        "        'Correct Answer': 'reference_answer',\n",
        "        'output_label': 'score',\n",
        "        'feedback': 'feedback',\n",
        "    }\n",
        "    \n",
        "    df = df.rename(columns=column_mapping)\n",
        "    \n",
        "    # Clean data\n",
        "    df = df.dropna(subset=['question', 'student_answer', 'score'])\n",
        "    df['score'] = df['score'].astype(int)\n",
        "    df = df[df['question'].str.strip() != '']\n",
        "    df = df[df['student_answer'].str.strip() != '']\n",
        "    \n",
        "    print(f\"Loaded {split} split: {len(df)} samples\")\n",
        "    print(f\"Score distribution:\\n{df['score'].value_counts().sort_index()}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load test dataset (unseen questions - primary test set)\n",
        "test_df = load_engsaf_split(CONFIG['dataset_dir'], 'unseen_question')\n",
        "print(f\"\\nTest dataset ready: {len(test_df)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation Metrics {#evaluation-metrics}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt template functions\n",
        "DEFAULT_RUBRIC = \"\"\"You are an expert grader evaluating student answers. Consider:\n",
        "1. Accuracy: Is the answer factually correct?\n",
        "2. Completeness: Does it address all parts of the question?\n",
        "3. Clarity: Is the answer well-structured and clear?\n",
        "4. Depth: Does it demonstrate understanding beyond surface level?\n",
        "\n",
        "Provide a score (0-5) and constructive feedback.\"\"\"\n",
        "\n",
        "def create_prompt_template(question, student_answer, rubric=None):\n",
        "    \"\"\"Create instruction-tuning prompt template.\"\"\"\n",
        "    if rubric is None:\n",
        "        rubric = DEFAULT_RUBRIC\n",
        "    \n",
        "    system_prompt = rubric\n",
        "    user_prompt = f\"\"\"Question: {question}\n",
        "Student Answer: {student_answer}\n",
        "\n",
        "Please grade this answer and provide feedback.\"\"\"\n",
        "    \n",
        "    return system_prompt, user_prompt\n",
        "\n",
        "def format_instruction(system_prompt, user_prompt, assistant_response=None):\n",
        "    \"\"\"Format instruction in Mistral's chat template format.\"\"\"\n",
        "    if assistant_response is None:\n",
        "        prompt = f\"<s>[INST] {system_prompt}\\n\\n{user_prompt} [/INST]\"\n",
        "    else:\n",
        "        prompt = f\"<s>[INST] {system_prompt}\\n\\n{user_prompt} [/INST] {assistant_response}</s>\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "print(\"Prompt template functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation utility functions\n",
        "\n",
        "def quadratic_weighted_kappa(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Quadratic Weighted Kappa (QWK) score.\n",
        "    QWK is the standard metric for automated essay scoring.\n",
        "    \"\"\"\n",
        "    min_score = min(min(y_true), min(y_pred))\n",
        "    max_score = max(max(y_true), max(y_pred))\n",
        "    \n",
        "    weights = np.zeros((max_score - min_score + 1, max_score - min_score + 1))\n",
        "    for i in range(len(weights)):\n",
        "        for j in range(len(weights)):\n",
        "            weights[i][j] = ((i - j) ** 2) / ((max_score - min_score) ** 2)\n",
        "    \n",
        "    kappa = cohen_kappa_score(y_true, y_pred, weights=weights)\n",
        "    return kappa\n",
        "\n",
        "def extract_score_from_response(response_text):\n",
        "    \"\"\"Extract score from model response.\"\"\"\n",
        "    patterns = [\n",
        "        r'Score:\\s*(\\d+)',\n",
        "        r'score:\\s*(\\d+)',\n",
        "        r'Score\\s*(\\d+)',\n",
        "        r'(\\d+)\\s*out\\s*of',\n",
        "        r'Grade:\\s*(\\d+)',\n",
        "    ]\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, response_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            try:\n",
        "                score = int(match.group(1))\n",
        "                score = max(0, min(5, score))\n",
        "                return score\n",
        "            except ValueError:\n",
        "                continue\n",
        "    \n",
        "    numbers = re.findall(r'\\d+', response_text)\n",
        "    if numbers:\n",
        "        try:\n",
        "            score = int(numbers[0])\n",
        "            score = max(0, min(5, score))\n",
        "            return score\n",
        "        except ValueError:\n",
        "            pass\n",
        "    \n",
        "    return None\n",
        "\n",
        "def extract_feedback_from_response(response_text):\n",
        "    \"\"\"Extract feedback text from model response.\"\"\"\n",
        "    patterns = [\n",
        "        r'Feedback:\\s*(.+?)(?:\\n|$)',\n",
        "        r'feedback:\\s*(.+?)(?:\\n|$)',\n",
        "        r'Feedback\\s*(.+?)(?:\\n|$)',\n",
        "    ]\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, response_text, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "    \n",
        "    lines = response_text.split('\\n')\n",
        "    if len(lines) > 1:\n",
        "        return '\\n'.join(lines[1:]).strip()\n",
        "    \n",
        "    return response_text.strip()\n",
        "\n",
        "print(\"Evaluation utility functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function for feedback quality\n",
        "\n",
        "def evaluate_feedback_quality(predicted_feedback, reference_feedback):\n",
        "    \"\"\"Evaluate feedback quality using BLEU, ROUGE, and BERTScore.\"\"\"\n",
        "    try:\n",
        "        from rouge_score import rouge_scorer\n",
        "        from bert_score import score as bert_score_fn\n",
        "        import nltk\n",
        "        \n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        \n",
        "        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "        \n",
        "        smooth = SmoothingFunction().method1\n",
        "        bleu = sentence_bleu(\n",
        "            [reference_feedback.split()],\n",
        "            predicted_feedback.split(),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        \n",
        "        rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        rouge_scores = rouge_scorer_obj.score(reference_feedback, predicted_feedback)\n",
        "        \n",
        "        P, R, F1 = bert_score_fn(\n",
        "            [predicted_feedback],\n",
        "            [reference_feedback],\n",
        "            lang='en',\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'bleu': bleu,\n",
        "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
        "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
        "            'rougeL': rouge_scores['rougeL'].fmeasure,\n",
        "            'bertscore_f1': F1.item()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating feedback: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Feedback evaluation functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset class for evaluation\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GradingDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=1024, rubric=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.rubric = rubric\n",
        "        \n",
        "        # Prepare data\n",
        "        self.data = []\n",
        "        for idx in range(len(self.df)):\n",
        "            row = self.df.iloc[idx]\n",
        "            \n",
        "            sys_prompt, user_prompt = create_prompt_template(\n",
        "                row['question'],\n",
        "                row['student_answer'],\n",
        "                rubric=self.rubric\n",
        "            )\n",
        "            \n",
        "            feedback = row.get('feedback', 'No feedback available.')\n",
        "            assistant_resp = f\"Score: {row['score']}\\nFeedback: {feedback}\"\n",
        "            full_text = format_instruction(sys_prompt, user_prompt, assistant_resp)\n",
        "            \n",
        "            self.data.append({\n",
        "                'text': full_text,\n",
        "                'score': row['score'],\n",
        "                'question': row['question'],\n",
        "                'student_answer': row['student_answer']\n",
        "            })\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            item['text'],\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': encoding['input_ids'].flatten(),\n",
        "            'score': item['score']\n",
        "        }\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = GradingDataset(test_df, tokenizer, max_length=CONFIG['max_length'])\n",
        "print(f\"Test dataset created: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation function\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_dataset, device='cuda', max_samples=None):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of the model on test set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Using device: {device} for evaluation\")\n",
        "    \n",
        "    predictions = []\n",
        "    true_scores = []\n",
        "    predicted_scores = []\n",
        "    predicted_feedbacks = []\n",
        "    reference_feedbacks = []\n",
        "    \n",
        "    # Limit samples if specified\n",
        "    eval_indices = range(len(test_dataset))\n",
        "    if max_samples:\n",
        "        eval_indices = eval_indices[:max_samples]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(eval_indices, desc=\"Evaluating\"):\n",
        "            item = test_dataset[idx]\n",
        "            \n",
        "            # Get question and answer from dataset\n",
        "            question = test_dataset.df.iloc[idx]['question']\n",
        "            student_answer = test_dataset.df.iloc[idx]['student_answer']\n",
        "            true_score = test_dataset.df.iloc[idx]['score']\n",
        "            reference_feedback = test_dataset.df.iloc[idx].get('feedback', '')\n",
        "            \n",
        "            # Create prompt\n",
        "            sys_prompt, user_prompt = create_prompt_template(question, student_answer)\n",
        "            prompt = format_instruction(sys_prompt, user_prompt)\n",
        "            \n",
        "            # Tokenize and move to device\n",
        "            inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=CONFIG['max_length'])\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Generate\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=CONFIG['max_new_tokens'],\n",
        "                temperature=CONFIG['temperature'],\n",
        "                top_p=CONFIG['top_p'],\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "            \n",
        "            # Decode\n",
        "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            \n",
        "            # Extract score and feedback\n",
        "            predicted_score = extract_score_from_response(generated_text)\n",
        "            predicted_feedback = extract_feedback_from_response(generated_text)\n",
        "            \n",
        "            predictions.append({\n",
        "                'question': question,\n",
        "                'student_answer': student_answer,\n",
        "                'true_score': true_score,\n",
        "                'predicted_score': predicted_score,\n",
        "                'reference_feedback': reference_feedback,\n",
        "                'predicted_feedback': predicted_feedback,\n",
        "                'full_response': generated_text\n",
        "            })\n",
        "            \n",
        "            if predicted_score is not None:\n",
        "                true_scores.append(true_score)\n",
        "                predicted_scores.append(predicted_score)\n",
        "                predicted_feedbacks.append(predicted_feedback)\n",
        "                reference_feedbacks.append(reference_feedback)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    results = {}\n",
        "    \n",
        "    if len(true_scores) > 0:\n",
        "        # Score metrics\n",
        "        results['qwk'] = quadratic_weighted_kappa(true_scores, predicted_scores)\n",
        "        results['cohen_kappa'] = cohen_kappa_score(true_scores, predicted_scores)\n",
        "        results['accuracy'] = accuracy_score(true_scores, predicted_scores)\n",
        "        results['confusion_matrix'] = confusion_matrix(true_scores, predicted_scores)\n",
        "        \n",
        "        # Feedback metrics (sample-based for efficiency)\n",
        "        if len(predicted_feedbacks) > 0:\n",
        "            sample_size = min(50, len(predicted_feedbacks))\n",
        "            sample_indices = np.random.choice(len(predicted_feedbacks), sample_size, replace=False)\n",
        "            \n",
        "            feedback_metrics = []\n",
        "            for idx in sample_indices:\n",
        "                metrics = evaluate_feedback_quality(\n",
        "                    predicted_feedbacks[idx],\n",
        "                    reference_feedbacks[idx]\n",
        "                )\n",
        "                if metrics:\n",
        "                    feedback_metrics.append(metrics)\n",
        "            \n",
        "            if feedback_metrics:\n",
        "                results['feedback_metrics'] = {\n",
        "                    'bleu': np.mean([m['bleu'] for m in feedback_metrics]),\n",
        "                    'rouge1': np.mean([m['rouge1'] for m in feedback_metrics]),\n",
        "                    'rouge2': np.mean([m['rouge2'] for m in feedback_metrics]),\n",
        "                    'rougeL': np.mean([m['rougeL'] for m in feedback_metrics]),\n",
        "                    'bertscore_f1': np.mean([m['bertscore_f1'] for m in feedback_metrics])\n",
        "                }\n",
        "    \n",
        "    return results, predictions\n",
        "\n",
        "print(\"Evaluation function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "print(\"Starting evaluation...\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Max samples: {CONFIG['max_samples'] if CONFIG['max_samples'] else 'All'}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "results, predictions = evaluate_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    test_dataset,\n",
        "    device=device,\n",
        "    max_samples=CONFIG['max_samples']\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Quadratic Weighted Kappa (QWK): {results['qwk']:.4f}\")\n",
        "print(f\"Cohen's Kappa: {results['cohen_kappa']:.4f}\")\n",
        "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(results['confusion_matrix'])\n",
        "\n",
        "if 'feedback_metrics' in results:\n",
        "    print(\"\\nFeedback Metrics:\")\n",
        "    for metric, value in results['feedback_metrics'].items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference Pipeline {#inference-pipeline}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference function for grading new answers\n",
        "\n",
        "def grade_answer(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    question,\n",
        "    student_answer,\n",
        "    rubric=None,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    max_new_tokens=256,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    Grade a student answer and generate feedback.\n",
        "    \n",
        "    Args:\n",
        "        model: Fine-tuned model\n",
        "        tokenizer: Tokenizer\n",
        "        question: The question text\n",
        "        student_answer: Student's answer\n",
        "        rubric: Optional custom rubric\n",
        "        temperature: Sampling temperature\n",
        "        top_p: Nucleus sampling parameter\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "        device: Device to run inference on\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'score' and 'feedback'\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Create prompt\n",
        "    sys_prompt, user_prompt = create_prompt_template(question, student_answer, rubric=rubric)\n",
        "    prompt = format_instruction(sys_prompt, user_prompt)\n",
        "    \n",
        "    # Tokenize and move to device\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=CONFIG['max_length'])\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract score and feedback\n",
        "    score = extract_score_from_response(generated_text)\n",
        "    feedback = extract_feedback_from_response(generated_text)\n",
        "    \n",
        "    return {\n",
        "        'score': score,\n",
        "        'feedback': feedback,\n",
        "        'full_response': generated_text\n",
        "    }\n",
        "\n",
        "print(\"Inference function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "example_question = \"Explain the process of photosynthesis.\"\n",
        "example_answer = \"Photosynthesis is when plants use sunlight to make food.\"\n",
        "\n",
        "result = grade_answer(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    example_question,\n",
        "    example_answer,\n",
        "    temperature=CONFIG['temperature'],\n",
        "    top_p=CONFIG['top_p'],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Example Grading:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Question: {example_question}\")\n",
        "print(f\"Answer: {example_answer}\")\n",
        "print(f\"\\nPredicted Score: {result['score']}\")\n",
        "print(f\"\\nGenerated Feedback:\\n{result['feedback']}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization & Analysis {#visualization}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, save_path=None):\n",
        "    \"\"\"Plot confusion matrix for score predictions.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=range(len(cm)),\n",
        "        yticklabels=range(len(cm))\n",
        "    )\n",
        "    plt.xlabel('Predicted Score')\n",
        "    plt.ylabel('True Score')\n",
        "    plt.title('Confusion Matrix - Score Predictions')\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix\n",
        "if len(predictions) > 0 and any(p['predicted_score'] is not None for p in predictions):\n",
        "    true_scores = [p['true_score'] for p in predictions if p['predicted_score'] is not None]\n",
        "    pred_scores = [p['predicted_score'] for p in predictions if p['predicted_score'] is not None]\n",
        "    plot_confusion_matrix(true_scores, pred_scores, save_path='confusion_matrix.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display example predictions\n",
        "\n",
        "def display_examples(predictions, n_examples=5, show_good=True, show_bad=True):\n",
        "    \"\"\"Display example predictions, both good and bad cases.\"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    if show_good:\n",
        "        good_examples = [\n",
        "            p for p in predictions\n",
        "            if p['predicted_score'] == p['true_score'] and p['predicted_score'] is not None\n",
        "        ]\n",
        "        if good_examples:\n",
        "            examples.extend(np.random.choice(good_examples, min(n_examples, len(good_examples)), replace=False))\n",
        "    \n",
        "    if show_bad:\n",
        "        bad_examples = [\n",
        "            p for p in predictions\n",
        "            if p['predicted_score'] is not None\n",
        "            and abs(p['predicted_score'] - p['true_score']) >= 2\n",
        "        ]\n",
        "        if bad_examples:\n",
        "            examples.extend(np.random.choice(bad_examples, min(n_examples, len(bad_examples)), replace=False))\n",
        "    \n",
        "    for i, ex in enumerate(examples[:n_examples * 2], 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Example {i}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Question: {ex['question']}\")\n",
        "        print(f\"\\nStudent Answer: {ex['student_answer']}\")\n",
        "        print(f\"\\nTrue Score: {ex['true_score']}\")\n",
        "        print(f\"Predicted Score: {ex['predicted_score']}\")\n",
        "        print(f\"\\nReference Feedback: {ex['reference_feedback'][:200]}...\")\n",
        "        print(f\"\\nGenerated Feedback: {ex['predicted_feedback'][:200]}...\")\n",
        "\n",
        "# Display examples\n",
        "display_examples(predictions, n_examples=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification report\n",
        "if len(predictions) > 0 and any(p['predicted_score'] is not None for p in predictions):\n",
        "    true_scores = [p['true_score'] for p in predictions if p['predicted_score'] is not None]\n",
        "    pred_scores = [p['predicted_score'] for p in predictions if p['predicted_score'] is not None]\n",
        "    \n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(true_scores, pred_scores))\n",
        "    \n",
        "    print(\"\\nScore Distribution:\")\n",
        "    print(f\"True scores: {pd.Series(true_scores).value_counts().sort_index()}\")\n",
        "    print(f\"Predicted scores: {pd.Series(pred_scores).value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Optional Enhancements {#optional-enhancements}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 RAG Integration Placeholder\n",
        "\n",
        "For future enhancement: Integrate Retrieval-Augmented Generation (RAG) to retrieve relevant course materials when grading answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG Integration Placeholder\n",
        "def retrieve_course_materials(question, top_k=3):\n",
        "    \"\"\"\n",
        "    Placeholder for RAG system to retrieve relevant course materials.\n",
        "    \n",
        "    Future implementation:\n",
        "    - Use embeddings to find relevant course content\n",
        "    - Retrieve top-k most relevant passages\n",
        "    - Include in prompt context\n",
        "    \"\"\"\n",
        "    return []\n",
        "\n",
        "print(\"RAG placeholder defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Chain-of-Thought Verification\n",
        "\n",
        "For future enhancement: Add Chain-of-Thought reasoning to make grading decisions more transparent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chain-of-Thought Verification Placeholder\n",
        "\n",
        "def grade_with_cot(model, tokenizer, question, answer, device='cuda'):\n",
        "    \"\"\"\n",
        "    Placeholder for Chain-of-Thought grading.\n",
        "    \n",
        "    Future implementation:\n",
        "    - Generate reasoning steps before final score\n",
        "    - Verify consistency of reasoning\n",
        "    - Use reasoning to improve score prediction\n",
        "    \"\"\"\n",
        "    return grade_answer(model, tokenizer, question, answer, device=device)\n",
        "\n",
        "print(\"Chain-of-Thought placeholder defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Ensemble with RoBERTa Baseline\n",
        "\n",
        "For future enhancement: Create ensemble model combining Mistral-7B with RoBERTa-based scoring model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensemble Placeholder\n",
        "\n",
        "def ensemble_grade(mistral_result, roberta_result, weights=[0.7, 0.3]):\n",
        "    \"\"\"\n",
        "    Placeholder for ensemble grading.\n",
        "    \n",
        "    Future implementation:\n",
        "    - Load fine-tuned RoBERTa model for scoring\n",
        "    - Combine predictions with weighted average\n",
        "    - Use ensemble for final score and feedback\n",
        "    \"\"\"\n",
        "    return mistral_result\n",
        "\n",
        "print(\"Ensemble placeholder defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to CSV\n",
        "results_df = pd.DataFrame(predictions)\n",
        "output_file = 'evaluation_results.csv'\n",
        "results_df.to_csv(output_file, index=False)\n",
        "print(f\"Results saved to {output_file}\")\n",
        "\n",
        "# Save metrics\n",
        "metrics = {\n",
        "    'qwk': float(results['qwk']),\n",
        "    'cohen_kappa': float(results['cohen_kappa']),\n",
        "    'accuracy': float(results['accuracy']),\n",
        "    'confusion_matrix': results['confusion_matrix'].tolist()\n",
        "}\n",
        "\n",
        "if 'feedback_metrics' in results:\n",
        "    metrics['feedback_metrics'] = {k: float(v) for k, v in results['feedback_metrics'].items()}\n",
        "\n",
        "with open('evaluation_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(\"Metrics saved to evaluation_metrics.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
