{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning Mistral-7B for Automatic Short Answer Grading\n",
        "\n",
        "## Table of Contents\n",
        "1. [Environment Setup](#environment-setup)\n",
        "2. [Data Loading & Preprocessing](#data-loading)\n",
        "3. [Model Configuration](#model-configuration)\n",
        "4. [Training Setup](#training-setup)\n",
        "5. [Evaluation Metrics](#evaluation-metrics)\n",
        "6. [Inference Pipeline](#inference-pipeline)\n",
        "7. [Visualization & Analysis](#visualization)\n",
        "8. [Optional Enhancements](#optional-enhancements)\n",
        "\n",
        "---\n",
        "\n",
        "### Requirements\n",
        "- **GPU**: P100 (16GB) or better\n",
        "- **Estimated Runtime**: 3-5 hours for full training\n",
        "- **Memory**: ~12-14GB GPU memory with 4-bit quantization\n",
        "\n",
        "### Troubleshooting Tips\n",
        "- If OOM errors occur, reduce batch size or increase gradient accumulation\n",
        "- Save checkpoints every 500 steps to avoid losing progress\n",
        "- Use Kaggle's \"Save Version\" feature regularly\n",
        "- If dataset loading fails, check Kaggle API authentication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup {#environment-setup}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers>=4.35.0 peft>=0.6.0 bitsandbytes>=0.41.0 datasets>=2.14.0 accelerate>=0.24.0 wandb scikit-learn nltk rouge-score bert-score torch torchvision torchaudio --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "from sklearn.metrics import (\n",
        "    cohen_kappa_score,\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional: wandb for experiment tracking\n",
        "try:\n",
        "    import wandb\n",
        "    WANDB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WANDB_AVAILABLE = False\n",
        "    print(\"wandb not available, skipping experiment tracking\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability and memory\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    print(\"\\nGPU Details:\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        print(f\"  GPU {i}: {props.name}\")\n",
        "        print(f\"    Total Memory: {props.total_memory / 1e9:.2f} GB\")\n",
        "        print(f\"    Compute Capability: {props.major}.{props.minor}\")\n",
        "    print(f\"\\nUsing device_map='auto' - model will be distributed across all available GPUs\")\n",
        "else:\n",
        "    print(\"Warning: No GPU detected. Training will be very slow on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle-specific setup\n",
        "# Check if running in Kaggle environment\n",
        "KAGGLE_ENV = os.path.exists('/kaggle/input')\n",
        "\n",
        "if KAGGLE_ENV:\n",
        "    print(\"Running in Kaggle environment\")\n",
        "    # Set up Kaggle output directory\n",
        "    OUTPUT_DIR = '/kaggle/working'\n",
        "    INPUT_DIR = '/kaggle/input'\n",
        "else:\n",
        "    print(\"Running in local environment\")\n",
        "    OUTPUT_DIR = './output'\n",
        "    INPUT_DIR = './input'\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Input directory: {INPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading & Preprocessing {#data-loading}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters - Easy to modify\n",
        "CONFIG = {\n",
        "    # Model config\n",
        "    'model_name': 'mistralai/Mistral-7B-v0.1',\n",
        "    'use_4bit': True,\n",
        "    'bnb_4bit_compute_dtype': 'float16',\n",
        "    'bnb_4bit_quant_type': 'nf4',\n",
        "    'use_nested_quant': False,\n",
        "    \n",
        "    # LoRA config\n",
        "    'lora_r': 32,  # Rank\n",
        "    'lora_alpha': 64,  # Scaling parameter\n",
        "    'lora_dropout': 0.05,\n",
        "    'lora_target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    \n",
        "    # Training config\n",
        "    'output_dir': os.path.join(OUTPUT_DIR, 'checkpoints'),\n",
        "    'num_train_epochs': 3,\n",
        "    'per_device_train_batch_size': 4,\n",
        "    'per_device_eval_batch_size': 4,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'learning_rate': 2e-4,\n",
        "    'lr_scheduler_type': 'cosine',\n",
        "    'warmup_steps': 100,\n",
        "    'logging_steps': 50,\n",
        "    'eval_steps': 500,\n",
        "    'save_steps': 500,\n",
        "    'save_total_limit': 3,\n",
        "    'fp16': True,\n",
        "    'bf16': False,  # Set to True if using A100 or newer GPUs\n",
        "    'gradient_checkpointing': True,\n",
        "    'dataloader_pin_memory': False,\n",
        "    \n",
        "    # Data config\n",
        "    'max_length': 1024,\n",
        "    'test_size': 0.15,\n",
        "    'val_size': 0.15,\n",
        "    'random_state': 42,\n",
        "    \n",
        "    # Inference config\n",
        "    'temperature': 0.7,\n",
        "    'top_p': 0.9,\n",
        "    'max_new_tokens': 256,\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load EngSAF dataset\n",
        "# The dataset is already split into train.csv, val.csv, unseen_question.csv, and unseen_answers.csv\n",
        "# Column mapping: Question -> question, Student Answer -> student_answer, \n",
        "#                 Correct Answer -> reference_answer, output_label -> score\n",
        "\n",
        "def load_engsaf_split(dataset_dir=None, split='train'):\n",
        "    \"\"\"\n",
        "    Load a specific split of the EngSAF dataset.\n",
        "    \n",
        "    Args:\n",
        "        dataset_dir: Directory containing the dataset files. If None, searches common paths.\n",
        "        split: Which split to load ('train', 'val', 'unseen_question', 'unseen_answers')\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with standardized column names\n",
        "    \"\"\"\n",
        "    if dataset_dir is None:\n",
        "        # Try common paths for the dataset directory\n",
        "        possible_dirs = [\n",
        "            'EngSAF dataset',\n",
        "            os.path.join(INPUT_DIR, 'engsaf-dataset'),\n",
        "            os.path.join(INPUT_DIR, 'engsaf'),\n",
        "            './EngSAF dataset',\n",
        "            os.path.join(INPUT_DIR, 'EngSAF dataset'),\n",
        "        ]\n",
        "        \n",
        "        for dir_path in possible_dirs:\n",
        "            if os.path.exists(dir_path):\n",
        "                dataset_dir = dir_path\n",
        "                break\n",
        "        \n",
        "        if dataset_dir is None:\n",
        "            raise FileNotFoundError(\"Could not find dataset directory. Please specify the path.\")\n",
        "    \n",
        "    # Map split names to file names\n",
        "    split_files = {\n",
        "        'train': 'train.csv',\n",
        "        'val': 'val.csv',\n",
        "        'validation': 'val.csv',\n",
        "        'unseen_question': 'unseen_question.csv',\n",
        "        'unseen_answers': 'unseen_answers.csv',\n",
        "        'test': 'unseen_question.csv'  # Default test set is unseen_question\n",
        "    }\n",
        "    \n",
        "    if split not in split_files:\n",
        "        raise ValueError(f\"Unknown split: {split}. Choose from {list(split_files.keys())}\")\n",
        "    \n",
        "    file_path = os.path.join(dataset_dir, split_files[split])\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Could not find {file_path}\")\n",
        "    \n",
        "    # Load CSV\n",
        "    df = pd.read_csv(file_path)\n",
        "    \n",
        "    # Map column names to standardized format\n",
        "    column_mapping = {\n",
        "        'Question': 'question',\n",
        "        'Student Answer': 'student_answer',\n",
        "        'Correct Answer': 'reference_answer',\n",
        "        'output_label': 'score',\n",
        "        'feedback': 'feedback',\n",
        "        'Question_id': 'question_id'  # Keep for reference but not required\n",
        "    }\n",
        "    \n",
        "    # Rename columns\n",
        "    df = df.rename(columns=column_mapping)\n",
        "    \n",
        "    # Validate required columns\n",
        "    required_cols = ['question', 'student_answer', 'reference_answer', 'score', 'feedback']\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing required columns: {missing_cols}. Available columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Clean data\n",
        "    df = df.dropna(subset=['question', 'student_answer', 'score'])\n",
        "    df['score'] = df['score'].astype(int)\n",
        "    \n",
        "    # Remove any rows with empty strings\n",
        "    df = df[df['question'].str.strip() != '']\n",
        "    df = df[df['student_answer'].str.strip() != '']\n",
        "    \n",
        "    print(f\"Loaded {split} split: {len(df)} samples\")\n",
        "    print(f\"Score distribution:\\n{df['score'].value_counts().sort_index()}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def load_all_engsaf_splits(dataset_dir=None):\n",
        "    \"\"\"\n",
        "    Load all splits of the EngSAF dataset.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (train_df, val_df, test_df_unseen_question, test_df_unseen_answers)\n",
        "    \"\"\"\n",
        "    train_df = load_engsaf_split(dataset_dir, 'train')\n",
        "    val_df = load_engsaf_split(dataset_dir, 'val')\n",
        "    test_df_unseen_question = load_engsaf_split(dataset_dir, 'unseen_question')\n",
        "    test_df_unseen_answers = load_engsaf_split(dataset_dir, 'unseen_answers')\n",
        "    \n",
        "    return train_df, val_df, test_df_unseen_question, test_df_unseen_answers\n",
        "\n",
        "# Load all dataset splits\n",
        "# The dataset is already split, so we load them directly\n",
        "dataset_dir = 'EngSAF dataset'  # Adjust path if needed\n",
        "\n",
        "# Check if dataset directory exists\n",
        "if not os.path.exists(dataset_dir):\n",
        "    # Try alternative paths\n",
        "    if os.path.exists(os.path.join(INPUT_DIR, 'EngSAF dataset')):\n",
        "        dataset_dir = os.path.join(INPUT_DIR, 'EngSAF dataset')\n",
        "    elif os.path.exists('./EngSAF dataset'):\n",
        "        dataset_dir = './EngSAF dataset'\n",
        "    else:\n",
        "        print(f\"Warning: Dataset directory not found. Please ensure 'EngSAF dataset' folder exists.\")\n",
        "        print(\"Available paths will be searched when loading.\")\n",
        "\n",
        "# Load all splits\n",
        "train_df, val_df, test_df_unseen_question, test_df_unseen_answers = load_all_engsaf_splits(dataset_dir)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Dataset Summary:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Training set: {len(train_df)} samples\")\n",
        "print(f\"Validation set: {len(val_df)} samples\")\n",
        "print(f\"Test set (unseen questions): {len(test_df_unseen_question)} samples\")\n",
        "print(f\"Test set (unseen answers): {len(test_df_unseen_answers)} samples\")\n",
        "print(\"\\nNote: Use test_df_unseen_question for final evaluation (unseen questions)\")\n",
        "print(\"      Use test_df_unseen_answers for additional evaluation (unseen answers)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset splits are already provided\n",
        "# The dataset comes pre-split into:\n",
        "# - train.csv: Training set\n",
        "# - val.csv: Validation set  \n",
        "# - unseen_question.csv: Test set with unseen questions (primary test set)\n",
        "# - unseen_answers.csv: Test set with unseen answers (additional test set)\n",
        "\n",
        "# The splits have already been loaded in the previous cell\n",
        "# train_df, val_df, test_df_unseen_question, test_df_unseen_answers\n",
        "\n",
        "# For training, we'll use:\n",
        "# - train_df for training\n",
        "# - val_df for validation\n",
        "# - test_df_unseen_question for final evaluation (unseen questions - most realistic)\n",
        "\n",
        "# Optional: You can also evaluate on test_df_unseen_answers for additional insights\n",
        "\n",
        "print(\"Dataset splits ready:\")\n",
        "print(f\"  Training: {len(train_df)} samples\")\n",
        "print(f\"  Validation: {len(val_df)} samples\")\n",
        "print(f\"  Test (unseen questions): {len(test_df_unseen_question)} samples\")\n",
        "print(f\"  Test (unseen answers): {len(test_df_unseen_answers)} samples\")\n",
        "\n",
        "# Set the primary test set for evaluation\n",
        "test_df = test_df_unseen_question  # Use unseen questions as primary test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rubric-conditioned prompt template\n",
        "# This template conditions the model on a grading rubric\n",
        "\n",
        "DEFAULT_RUBRIC = \"\"\"You are an expert grader evaluating student answers. Consider:\n",
        "1. Accuracy: Is the answer factually correct?\n",
        "2. Completeness: Does it address all parts of the question?\n",
        "3. Clarity: Is the answer well-structured and clear?\n",
        "4. Depth: Does it demonstrate understanding beyond surface level?\n",
        "\n",
        "Provide a score (0-5) and constructive feedback.\"\"\"\n",
        "\n",
        "def create_prompt_template(question, student_answer, rubric=None, system_prompt=None):\n",
        "    \"\"\"\n",
        "    Create instruction-tuning prompt template.\n",
        "    \n",
        "    Format:\n",
        "    <s>[INST] System: {system_prompt}\n",
        "    \n",
        "    User: Question: {question}\n",
        "    Student Answer: {student_answer}\n",
        "    \n",
        "    Please grade this answer and provide feedback. [/INST]\n",
        "    Assistant: Score: {score}\n",
        "    Feedback: {feedback} </s>\n",
        "    \"\"\"\n",
        "    if rubric is None:\n",
        "        rubric = DEFAULT_RUBRIC\n",
        "    \n",
        "    if system_prompt is None:\n",
        "        system_prompt = rubric\n",
        "    \n",
        "    user_prompt = f\"\"\"Question: {question}\n",
        "Student Answer: {student_answer}\n",
        "\n",
        "Please grade this answer and provide feedback.\"\"\"\n",
        "    \n",
        "    return system_prompt, user_prompt\n",
        "\n",
        "def format_instruction(system_prompt, user_prompt, assistant_response=None):\n",
        "    \"\"\"\n",
        "    Format instruction in Mistral's chat template format.\n",
        "    \"\"\"\n",
        "    if assistant_response is None:\n",
        "        # For inference\n",
        "        prompt = f\"<s>[INST] {system_prompt}\\n\\n{user_prompt} [/INST]\"\n",
        "    else:\n",
        "        # For training\n",
        "        prompt = f\"<s>[INST] {system_prompt}\\n\\n{user_prompt} [/INST] {assistant_response}</s>\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def create_assistant_response(score, feedback):\n",
        "    \"\"\"\n",
        "    Create formatted assistant response with score and feedback.\n",
        "    \"\"\"\n",
        "    return f\"Score: {score}\\nFeedback: {feedback}\"\n",
        "\n",
        "# Example usage\n",
        "example_question = \"What is photosynthesis?\"\n",
        "example_answer = \"It's when plants make food using sunlight.\"\n",
        "example_score = 3\n",
        "example_feedback = \"Your answer captures the basic concept but lacks detail. Photosynthesis involves converting light energy into chemical energy, specifically glucose, using carbon dioxide and water.\"\n",
        "\n",
        "sys_prompt, user_prompt = create_prompt_template(example_question, example_answer)\n",
        "assistant_resp = create_assistant_response(example_score, example_feedback)\n",
        "formatted = format_instruction(sys_prompt, user_prompt, assistant_resp)\n",
        "\n",
        "print(\"Example prompt format:\")\n",
        "print(\"=\" * 80)\n",
        "print(formatted)\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset class for instruction tuning\n",
        "\n",
        "class GradingDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=1024, rubric=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.rubric = rubric\n",
        "        \n",
        "        # Prepare data\n",
        "        self.data = []\n",
        "        for idx in range(len(self.df)):\n",
        "            row = self.df.iloc[idx]\n",
        "            \n",
        "            # Create prompts\n",
        "            sys_prompt, user_prompt = create_prompt_template(\n",
        "                row['question'],\n",
        "                row['student_answer'],\n",
        "                rubric=self.rubric\n",
        "            )\n",
        "            \n",
        "            # Create assistant response\n",
        "            feedback = row.get('feedback', 'No feedback available.')\n",
        "            assistant_resp = create_assistant_response(row['score'], feedback)\n",
        "            \n",
        "            # Format full instruction\n",
        "            full_text = format_instruction(sys_prompt, user_prompt, assistant_resp)\n",
        "            \n",
        "            self.data.append({\n",
        "                'text': full_text,\n",
        "                'score': row['score'],\n",
        "                'question': row['question'],\n",
        "                'student_answer': row['student_answer']\n",
        "            })\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            item['text'],\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': encoding['input_ids'].flatten(),  # For causal LM, labels = input_ids\n",
        "            'score': item['score']\n",
        "        }\n",
        "\n",
        "print(\"Dataset class defined. Will be instantiated after tokenizer is loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Configuration {#model-configuration}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "# Set padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure 4-bit quantization for memory efficiency\n",
        "if CONFIG['use_4bit']:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=CONFIG['bnb_4bit_quant_type'],\n",
        "        bnb_4bit_compute_dtype=getattr(torch, CONFIG['bnb_4bit_compute_dtype']),\n",
        "        bnb_4bit_use_double_quant=CONFIG['use_nested_quant'],\n",
        "    )\n",
        "    print(\"4-bit quantization configured\")\n",
        "else:\n",
        "    bnb_config = None\n",
        "    print(\"Using full precision (not recommended for Kaggle P100)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base model with quantization\n",
        "print(\"Loading Mistral-7B model (this may take a few minutes)...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG['model_name'],\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if CONFIG['use_4bit'] else torch.float32\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "if CONFIG['gradient_checkpointing']:\n",
        "    model.gradient_checkpointing_enable()\n",
        "    print(\"Gradient checkpointing enabled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare model for k-bit training\n",
        "if CONFIG['use_4bit']:\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    print(\"Model prepared for k-bit training\")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=CONFIG['lora_r'],\n",
        "    lora_alpha=CONFIG['lora_alpha'],\n",
        "    target_modules=CONFIG['lora_target_modules'],\n",
        "    lora_dropout=CONFIG['lora_dropout'],\n",
        "    bias='none',\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"LoRA configuration:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Save LoRA config for reference (convert to JSON-serializable format)\n",
        "lora_config_dict = {\n",
        "    'r': lora_config.r,\n",
        "    'lora_alpha': lora_config.lora_alpha,\n",
        "    'target_modules': list(lora_config.target_modules) if isinstance(lora_config.target_modules, (set, tuple)) else lora_config.target_modules,\n",
        "    'lora_dropout': lora_config.lora_dropout,\n",
        "    'bias': lora_config.bias,\n",
        "    'task_type': str(lora_config.task_type),\n",
        "    'peft_type': str(lora_config.peft_type) if hasattr(lora_config, 'peft_type') else 'LORA',\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'lora_config.json'), 'w') as f:\n",
        "    json.dump(lora_config_dict, f, indent=2)\n",
        "\n",
        "print(f\"LoRA config saved to {os.path.join(OUTPUT_DIR, 'lora_config.json')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Setup {#training-setup}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create PyTorch datasets from the loaded DataFrames\n",
        "# Note: This requires the tokenizer to be loaded first (from Model Configuration section)\n",
        "\n",
        "# Check if tokenizer is available\n",
        "try:\n",
        "    train_dataset = GradingDataset(train_df, tokenizer, max_length=CONFIG['max_length'])\n",
        "    val_dataset = GradingDataset(val_df, tokenizer, max_length=CONFIG['max_length'])\n",
        "    test_dataset = GradingDataset(test_df, tokenizer, max_length=CONFIG['max_length'])\n",
        "    \n",
        "    # Optional: Also create dataset for unseen_answers test set\n",
        "    test_dataset_unseen_answers = GradingDataset(test_df_unseen_answers, tokenizer, max_length=CONFIG['max_length'])\n",
        "    \n",
        "    print(\"Datasets created successfully:\")\n",
        "    print(f\"  Training dataset: {len(train_dataset)} samples\")\n",
        "    print(f\"  Validation dataset: {len(val_dataset)} samples\")\n",
        "    print(f\"  Test dataset (unseen questions): {len(test_dataset)} samples\")\n",
        "    print(f\"  Test dataset (unseen answers): {len(test_dataset_unseen_answers)} samples\")\n",
        "except NameError:\n",
        "    print(\"Note: Tokenizer not yet loaded. Please run the Model Configuration cells first.\")\n",
        "    print(\"After loading the tokenizer, this cell will automatically create the datasets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom data collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, not masked LM\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CONFIG['output_dir'],\n",
        "    num_train_epochs=CONFIG['num_train_epochs'],\n",
        "    per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
        "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    lr_scheduler_type=CONFIG['lr_scheduler_type'],\n",
        "    warmup_steps=CONFIG['warmup_steps'],\n",
        "    logging_steps=CONFIG['logging_steps'],\n",
        "    eval_steps=CONFIG['eval_steps'],\n",
        "    save_steps=CONFIG['save_steps'],\n",
        "    save_total_limit=CONFIG['save_total_limit'],\n",
        "    fp16=CONFIG['fp16'],\n",
        "    bf16=CONFIG['bf16'],\n",
        "    gradient_checkpointing=CONFIG['gradient_checkpointing'],\n",
        "    dataloader_pin_memory=CONFIG['dataloader_pin_memory'],\n",
        "    eval_strategy='steps',  # Changed from evaluation_strategy to eval_strategy (newer transformers API)\n",
        "    save_strategy='steps',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        "    report_to='wandb' if WANDB_AVAILABLE else 'none',\n",
        "    run_name='mistral-engsaf-finetune',\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize wandb (optional)\n",
        "if WANDB_AVAILABLE:\n",
        "    wandb.init(\n",
        "        project='mistral-engsaf-grading',\n",
        "        name='mistral-7b-lora-finetune',\n",
        "        config=CONFIG\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Trainer with evaluation metrics\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "class MetricsCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            print(f\"\\nEvaluation at step {state.global_step}:\")\n",
        "            for key, value in logs.items():\n",
        "                print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# Create trainer\n",
        "# Uncomment when datasets are ready:\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "#     data_collator=data_collator,\n",
        "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=3), MetricsCallback()],\n",
        "# )\n",
        "\n",
        "print(\"Trainer ready. Uncomment to start training when data is loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "# Uncomment to train:\n",
        "# print(\"Starting training...\")\n",
        "# trainer.train()\n",
        "# \n",
        "# # Save final model\n",
        "# final_model_path = os.path.join(OUTPUT_DIR, 'final_model')\n",
        "# trainer.save_model(final_model_path)\n",
        "# tokenizer.save_pretrained(final_model_path)\n",
        "# print(f\"Model saved to {final_model_path}\")\n",
        "\n",
        "print(\"Training code ready. Uncomment to start training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation Metrics {#evaluation-metrics}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation metrics\n",
        "\n",
        "def quadratic_weighted_kappa(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Quadratic Weighted Kappa (QWK) score.\n",
        "    QWK is the standard metric for automated essay scoring.\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import cohen_kappa_score\n",
        "    \n",
        "    # Get min and max scores\n",
        "    min_score = min(min(y_true), min(y_pred))\n",
        "    max_score = max(max(y_true), max(y_pred))\n",
        "    \n",
        "    # Create weight matrix\n",
        "    weights = np.zeros((max_score - min_score + 1, max_score - min_score + 1))\n",
        "    for i in range(len(weights)):\n",
        "        for j in range(len(weights)):\n",
        "            weights[i][j] = ((i - j) ** 2) / ((max_score - min_score) ** 2)\n",
        "    \n",
        "    # Calculate QWK\n",
        "    kappa = cohen_kappa_score(y_true, y_pred, weights=weights)\n",
        "    return kappa\n",
        "\n",
        "def extract_score_from_response(response_text):\n",
        "    \"\"\"\n",
        "    Extract score from model response.\n",
        "    Looks for patterns like 'Score: 3' or 'score: 3'.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Try to find score pattern\n",
        "    patterns = [\n",
        "        r'Score:\\s*(\\d+)',\n",
        "        r'score:\\s*(\\d+)',\n",
        "        r'Score\\s*(\\d+)',\n",
        "        r'(\\d+)\\s*out\\s*of',\n",
        "        r'Grade:\\s*(\\d+)',\n",
        "    ]\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, response_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            try:\n",
        "                score = int(match.group(1))\n",
        "                # Clamp to reasonable range (0-5)\n",
        "                score = max(0, min(5, score))\n",
        "                return score\n",
        "            except ValueError:\n",
        "                continue\n",
        "    \n",
        "    # Fallback: try to find first number\n",
        "    numbers = re.findall(r'\\d+', response_text)\n",
        "    if numbers:\n",
        "        try:\n",
        "            score = int(numbers[0])\n",
        "            score = max(0, min(5, score))\n",
        "            return score\n",
        "        except ValueError:\n",
        "            pass\n",
        "    \n",
        "    # Default fallback\n",
        "    return None\n",
        "\n",
        "def extract_feedback_from_response(response_text):\n",
        "    \"\"\"\n",
        "    Extract feedback text from model response.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Try to find feedback section\n",
        "    patterns = [\n",
        "        r'Feedback:\\s*(.+?)(?:\\n|$)',\n",
        "        r'feedback:\\s*(.+?)(?:\\n|$)',\n",
        "        r'Feedback\\s*(.+?)(?:\\n|$)',\n",
        "    ]\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, response_text, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "    \n",
        "    # If no pattern found, return everything after first newline\n",
        "    lines = response_text.split('\\n')\n",
        "    if len(lines) > 1:\n",
        "        return '\\n'.join(lines[1:]).strip()\n",
        "    \n",
        "    return response_text.strip()\n",
        "\n",
        "print(\"Evaluation utility functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function for feedback quality\n",
        "\n",
        "def evaluate_feedback_quality(predicted_feedback, reference_feedback):\n",
        "    \"\"\"\n",
        "    Evaluate feedback quality using BLEU, ROUGE, and BERTScore.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from rouge_score import rouge_scorer\n",
        "        from bert_score import score as bert_score_fn\n",
        "        import nltk\n",
        "        \n",
        "        # Download required NLTK data\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "        \n",
        "        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "        \n",
        "        # BLEU score\n",
        "        smooth = SmoothingFunction().method1\n",
        "        bleu = sentence_bleu(\n",
        "            [reference_feedback.split()],\n",
        "            predicted_feedback.split(),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        \n",
        "        # ROUGE scores\n",
        "        rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        rouge_scores = rouge_scorer_obj.score(reference_feedback, predicted_feedback)\n",
        "        \n",
        "        # BERTScore\n",
        "        P, R, F1 = bert_score_fn(\n",
        "            [predicted_feedback],\n",
        "            [reference_feedback],\n",
        "            lang='en',\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'bleu': bleu,\n",
        "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
        "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
        "            'rougeL': rouge_scores['rougeL'].fmeasure,\n",
        "            'bertscore_f1': F1.item()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating feedback: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Feedback evaluation functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation function\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_dataset, device='cuda', max_samples=None):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of the model on test set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    true_scores = []\n",
        "    predicted_scores = []\n",
        "    predicted_feedbacks = []\n",
        "    reference_feedbacks = []\n",
        "    \n",
        "    # Limit samples if specified\n",
        "    eval_indices = range(len(test_dataset))\n",
        "    if max_samples:\n",
        "        eval_indices = eval_indices[:max_samples]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(eval_indices, desc=\"Evaluating\"):\n",
        "            item = test_dataset[idx]\n",
        "            \n",
        "            # Get question and answer from dataset\n",
        "            question = test_dataset.df.iloc[idx]['question']\n",
        "            student_answer = test_dataset.df.iloc[idx]['student_answer']\n",
        "            true_score = test_dataset.df.iloc[idx]['score']\n",
        "            reference_feedback = test_dataset.df.iloc[idx].get('feedback', '')\n",
        "            \n",
        "            # Create prompt\n",
        "            sys_prompt, user_prompt = create_prompt_template(question, student_answer)\n",
        "            prompt = format_instruction(sys_prompt, user_prompt)\n",
        "            \n",
        "            # Tokenize\n",
        "            inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=CONFIG['max_length'])\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Generate\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=CONFIG['max_new_tokens'],\n",
        "                temperature=CONFIG['temperature'],\n",
        "                top_p=CONFIG['top_p'],\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "            \n",
        "            # Decode\n",
        "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            \n",
        "            # Extract score and feedback\n",
        "            predicted_score = extract_score_from_response(generated_text)\n",
        "            predicted_feedback = extract_feedback_from_response(generated_text)\n",
        "            \n",
        "            predictions.append({\n",
        "                'question': question,\n",
        "                'student_answer': student_answer,\n",
        "                'true_score': true_score,\n",
        "                'predicted_score': predicted_score,\n",
        "                'reference_feedback': reference_feedback,\n",
        "                'predicted_feedback': predicted_feedback,\n",
        "                'full_response': generated_text\n",
        "            })\n",
        "            \n",
        "            if predicted_score is not None:\n",
        "                true_scores.append(true_score)\n",
        "                predicted_scores.append(predicted_score)\n",
        "                predicted_feedbacks.append(predicted_feedback)\n",
        "                reference_feedbacks.append(reference_feedback)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    results = {}\n",
        "    \n",
        "    if len(true_scores) > 0:\n",
        "        # Score metrics\n",
        "        results['qwk'] = quadratic_weighted_kappa(true_scores, predicted_scores)\n",
        "        results['cohen_kappa'] = cohen_kappa_score(true_scores, predicted_scores)\n",
        "        results['accuracy'] = accuracy_score(true_scores, predicted_scores)\n",
        "        results['confusion_matrix'] = confusion_matrix(true_scores, predicted_scores)\n",
        "        \n",
        "        # Feedback metrics (sample-based for efficiency)\n",
        "        if len(predicted_feedbacks) > 0:\n",
        "            sample_size = min(50, len(predicted_feedbacks))  # Sample for efficiency\n",
        "            sample_indices = np.random.choice(len(predicted_feedbacks), sample_size, replace=False)\n",
        "            \n",
        "            feedback_metrics = []\n",
        "            for idx in sample_indices:\n",
        "                metrics = evaluate_feedback_quality(\n",
        "                    predicted_feedbacks[idx],\n",
        "                    reference_feedbacks[idx]\n",
        "                )\n",
        "                if metrics:\n",
        "                    feedback_metrics.append(metrics)\n",
        "            \n",
        "            if feedback_metrics:\n",
        "                results['feedback_metrics'] = {\n",
        "                    'bleu': np.mean([m['bleu'] for m in feedback_metrics]),\n",
        "                    'rouge1': np.mean([m['rouge1'] for m in feedback_metrics]),\n",
        "                    'rouge2': np.mean([m['rouge2'] for m in feedback_metrics]),\n",
        "                    'rougeL': np.mean([m['rougeL'] for m in feedback_metrics]),\n",
        "                    'bertscore_f1': np.mean([m['bertscore_f1'] for m in feedback_metrics])\n",
        "                }\n",
        "    \n",
        "    return results, predictions\n",
        "\n",
        "print(\"Evaluation function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "# Uncomment when model and test dataset are ready:\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = model.to(device)\n",
        "# \n",
        "# results, predictions = evaluate_model(\n",
        "#     model,\n",
        "#     tokenizer,\n",
        "#     test_dataset,\n",
        "#     device=device,\n",
        "#     max_samples=100  # Limit for faster evaluation\n",
        "# )\n",
        "# \n",
        "# print(\"\\nEvaluation Results:\")\n",
        "# print(f\"Quadratic Weighted Kappa (QWK): {results['qwk']:.4f}\")\n",
        "# print(f\"Cohen's Kappa: {results['cohen_kappa']:.4f}\")\n",
        "# print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
        "# \n",
        "# if 'feedback_metrics' in results:\n",
        "#     print(\"\\nFeedback Metrics:\")\n",
        "#     for metric, value in results['feedback_metrics'].items():\n",
        "#         print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"Evaluation code ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference Pipeline {#inference-pipeline}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference function for grading new answers\n",
        "\n",
        "def grade_answer(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    question,\n",
        "    student_answer,\n",
        "    rubric=None,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    max_new_tokens=256,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    Grade a student answer and generate feedback.\n",
        "    \n",
        "    Args:\n",
        "        model: Fine-tuned model\n",
        "        tokenizer: Tokenizer\n",
        "        question: The question text\n",
        "        student_answer: Student's answer\n",
        "        rubric: Optional custom rubric\n",
        "        temperature: Sampling temperature\n",
        "        top_p: Nucleus sampling parameter\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "        device: Device to run inference on\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'score' and 'feedback'\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Create prompt\n",
        "    sys_prompt, user_prompt = create_prompt_template(question, student_answer, rubric=rubric)\n",
        "    prompt = format_instruction(sys_prompt, user_prompt)\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=CONFIG['max_length'])\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract score and feedback\n",
        "    score = extract_score_from_response(generated_text)\n",
        "    feedback = extract_feedback_from_response(generated_text)\n",
        "    \n",
        "    return {\n",
        "        'score': score,\n",
        "        'feedback': feedback,\n",
        "        'full_response': generated_text\n",
        "    }\n",
        "\n",
        "print(\"Inference function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "# Uncomment when model is loaded:\n",
        "# \n",
        "# example_question = \"Explain the process of photosynthesis.\"\n",
        "# example_answer = \"Photosynthesis is when plants use sunlight to make food.\"\n",
        "# \n",
        "# result = grade_answer(\n",
        "#     model,\n",
        "#     tokenizer,\n",
        "#     example_question,\n",
        "#     example_answer,\n",
        "#     temperature=CONFIG['temperature'],\n",
        "#     top_p=CONFIG['top_p'],\n",
        "#     device='cuda'\n",
        "# )\n",
        "# \n",
        "# print(\"Example Grading:\")\n",
        "# print(f\"Question: {example_question}\")\n",
        "# print(f\"Answer: {example_answer}\")\n",
        "# print(f\"\\nPredicted Score: {result['score']}\")\n",
        "# print(f\"\\nGenerated Feedback:\\n{result['feedback']}\")\n",
        "\n",
        "print(\"Example inference code ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model checkpoint for Kaggle output\n",
        "# This ensures the model persists after session ends\n",
        "\n",
        "def save_model_checkpoint(model, tokenizer, output_path):\n",
        "    \"\"\"\n",
        "    Save model and tokenizer to output path.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    \n",
        "    # Save PEFT model\n",
        "    model.save_pretrained(output_path)\n",
        "    tokenizer.save_pretrained(output_path)\n",
        "    \n",
        "    # Save config\n",
        "    with open(os.path.join(output_path, 'training_config.json'), 'w') as f:\n",
        "        json.dump(CONFIG, f, indent=2)\n",
        "    \n",
        "    print(f\"Model saved to {output_path}\")\n",
        "\n",
        "# Uncomment to save:\n",
        "# checkpoint_path = os.path.join(OUTPUT_DIR, 'final_checkpoint')\n",
        "# save_model_checkpoint(model, tokenizer, checkpoint_path)\n",
        "\n",
        "print(\"Model saving function ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization & Analysis {#visualization}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training/validation loss curves\n",
        "# This requires training logs - uncomment after training\n",
        "\n",
        "def plot_training_curves(log_history, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss curves.\n",
        "    \"\"\"\n",
        "    train_losses = [log['loss'] for log in log_history if 'loss' in log and 'eval_loss' not in str(log)]\n",
        "    eval_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
        "    steps = [log['step'] for log in log_history if 'loss' in log]\n",
        "    \n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(steps[:len(train_losses)], train_losses, label='Train Loss', marker='o')\n",
        "    if eval_losses:\n",
        "        eval_steps = [log['step'] for log in log_history if 'eval_loss' in log]\n",
        "        plt.plot(eval_steps, eval_losses, label='Val Loss', marker='s')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "print(\"Training curve plotting function defined.\")\n",
        "# Uncomment after training:\n",
        "# plot_training_curves(trainer.state.log_history, save_path=os.path.join(OUTPUT_DIR, 'training_curves.png'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix for score predictions.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=range(len(cm)),\n",
        "        yticklabels=range(len(cm))\n",
        "    )\n",
        "    plt.xlabel('Predicted Score')\n",
        "    plt.ylabel('True Score')\n",
        "    plt.title('Confusion Matrix - Score Predictions')\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "print(\"Confusion matrix plotting function defined.\")\n",
        "# Uncomment after evaluation:\n",
        "# plot_confusion_matrix(true_scores, predicted_scores, save_path=os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display example predictions\n",
        "\n",
        "def display_examples(predictions, n_examples=5, show_good=True, show_bad=True):\n",
        "    \"\"\"\n",
        "    Display example predictions, both good and bad cases.\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    if show_good:\n",
        "        # Find examples where prediction matches true score\n",
        "        good_examples = [\n",
        "            p for p in predictions\n",
        "            if p['predicted_score'] == p['true_score'] and p['predicted_score'] is not None\n",
        "        ]\n",
        "        if good_examples:\n",
        "            examples.extend(np.random.choice(good_examples, min(n_examples, len(good_examples)), replace=False))\n",
        "    \n",
        "    if show_bad:\n",
        "        # Find examples with large prediction errors\n",
        "        bad_examples = [\n",
        "            p for p in predictions\n",
        "            if p['predicted_score'] is not None\n",
        "            and abs(p['predicted_score'] - p['true_score']) >= 2\n",
        "        ]\n",
        "        if bad_examples:\n",
        "            examples.extend(np.random.choice(bad_examples, min(n_examples, len(bad_examples)), replace=False))\n",
        "    \n",
        "    for i, ex in enumerate(examples[:n_examples * 2], 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Example {i}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Question: {ex['question']}\")\n",
        "        print(f\"\\nStudent Answer: {ex['student_answer']}\")\n",
        "        print(f\"\\nTrue Score: {ex['true_score']}\")\n",
        "        print(f\"Predicted Score: {ex['predicted_score']}\")\n",
        "        print(f\"\\nReference Feedback: {ex['reference_feedback'][:200]}...\")\n",
        "        print(f\"\\nGenerated Feedback: {ex['predicted_feedback'][:200]}...\")\n",
        "\n",
        "print(\"Example display function defined.\")\n",
        "# Uncomment after evaluation:\n",
        "# display_examples(predictions, n_examples=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Optional Enhancements {#optional-enhancements}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 RAG Integration Placeholder\n",
        "\n",
        "For future enhancement: Integrate Retrieval-Augmented Generation (RAG) to retrieve relevant course materials when grading answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG Integration Placeholder\n",
        "# This would retrieve relevant course materials to enhance grading context\n",
        "\n",
        "def retrieve_course_materials(question, top_k=3):\n",
        "    \"\"\"\n",
        "    Placeholder for RAG system to retrieve relevant course materials.\n",
        "    \n",
        "    Future implementation:\n",
        "    - Use embeddings to find relevant course content\n",
        "    - Retrieve top-k most relevant passages\n",
        "    - Include in prompt context\n",
        "    \"\"\"\n",
        "    # Placeholder\n",
        "    return []\n",
        "\n",
        "print(\"RAG placeholder defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Chain-of-Thought Verification\n",
        "\n",
        "For future enhancement: Add Chain-of-Thought reasoning to make grading decisions more transparent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chain-of-Thought Verification Placeholder\n",
        "\n",
        "def grade_with_cot(model, tokenizer, question, answer, device='cuda'):\n",
        "    \"\"\"\n",
        "    Placeholder for Chain-of-Thought grading.\n",
        "    \n",
        "    Future implementation:\n",
        "    - Generate reasoning steps before final score\n",
        "    - Verify consistency of reasoning\n",
        "    - Use reasoning to improve score prediction\n",
        "    \"\"\"\n",
        "    # Placeholder - would use multi-step prompting\n",
        "    return grade_answer(model, tokenizer, question, answer, device=device)\n",
        "\n",
        "print(\"Chain-of-Thought placeholder defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Ensemble with RoBERTa Baseline\n",
        "\n",
        "For future enhancement: Create ensemble model combining Mistral-7B with RoBERTa-based scoring model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensemble Placeholder\n",
        "\n",
        "def ensemble_grade(mistral_result, roberta_result, weights=[0.7, 0.3]):\n",
        "    \"\"\"\n",
        "    Placeholder for ensemble grading.\n",
        "    \n",
        "    Future implementation:\n",
        "    - Load fine-tuned RoBERTa model for scoring\n",
        "    - Combine predictions with weighted average\n",
        "    - Use ensemble for final score and feedback\n",
        "    \"\"\"\n",
        "    # Placeholder\n",
        "    return mistral_result\n",
        "\n",
        "print(\"Ensemble placeholder defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook provides a complete pipeline for fine-tuning Mistral-7B on the EngSAF dataset for automatic short answer grading.\n",
        "\n",
        "### Key Features:\n",
        "- 4-bit quantization for memory efficiency\n",
        "- LoRA/PEFT for parameter-efficient fine-tuning\n",
        "- Unseen-question split methodology\n",
        "- Comprehensive evaluation metrics (QWK, BLEU, ROUGE, BERTScore)\n",
        "- Full inference pipeline\n",
        "- Visualization and analysis tools\n",
        "\n",
        "### Next Steps:\n",
        "1. Load your EngSAF dataset\n",
        "2. Uncomment training cells\n",
        "3. Run training and evaluation\n",
        "4. Analyze results and iterate\n",
        "\n",
        "### Notes:\n",
        "- Remember to save checkpoints regularly (every 500 steps)\n",
        "- Monitor GPU memory usage\n",
        "- Use Kaggle's \"Save Version\" feature to persist work\n",
        "- Adjust hyperparameters based on your dataset size and characteristics\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
